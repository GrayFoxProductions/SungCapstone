Traceback (most recent call last):
  File "/home/hadoop/etl.py", line 106, in <module>
    main()
  File "/home/hadoop/etl.py", line 100, in main
    process_immigration_data(spark, input_data, output_data, immigration_file_name, temperature_file_name, mapping_file)
  File "/home/hadoop/etl.py", line 40, in process_immigration_data
    immigration_df = spark.read.load(immigration_file)
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 307, in load
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/usr/lib/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o228.load.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5) (ip-172-31-58-52.us-west-2.compute.internal executor 3): org.apache.spark.SparkException: Exception thrown in awaitResult:
        at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
        at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:1186)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:1236)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:1228)
        at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:874)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:874)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:333)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
        at org.apache.spark.scheduler.Task.run(Task.scala:152)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: s3://sungs3bucket/i94_apr16_sub.sas7bdat. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.
        at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1058)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:1199)
        at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
        at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
        at scala.util.Success.$anonfun$map$1(Try.scala:255)
        at scala.util.Success.map(Try.scala:213)
        at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
        at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
        at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
        at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
        at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
        at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
        at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Caused by: org.apache.parquet.io.InvalidParquetFormatException: s3://sungs3bucket/i94_apr16_sub.sas7bdat is not a Parquet file. Expected magic number at tail, but found [0, 0, 0, 0]
        at org.apache.parquet.hadoop.ParquetFileReader.getEncryptedFooterMode(ParquetFileReader.java:689)
        at org.apache.parquet.hadoop.ParquetFileReader.readInterFooter(ParquetFileReader.java:583)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:678)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:673)
        at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:935)
        at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:922)
        at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:760)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:198)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:83)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:1193)
        ... 14 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3083)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3019)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3018)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3018)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1324)
        at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1324)
        at scala.Option.foreach(Option.scala:407)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1324)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3301)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3235)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3224)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1047)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2499)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2520)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2539)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:2564)
        at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1065)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:152)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:113)
        at org.apache.spark.rdd.RDD.withScope(RDD.scala:426)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:1064)
        at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.mergeSchemasInParallel(SchemaMergeUtils.scala:74)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.mergeSchemasInParallel(ParquetFileFormat.scala:1240)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$.inferSchema(ParquetUtils.scala:132)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.inferSchema(ParquetFileFormat.scala:101)
        at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:209)
        at scala.Option.orElse(Option.scala:447)
        at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:206)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:432)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:345)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:278)
        at org.apache.spark.sql.DataFrameReader.$anonfun$load$1(DataFrameReader.scala:210)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)
        at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:569)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
        at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
        at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult:
        at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
        at org.apache.spark.util.ThreadUtils$.parmap(ThreadUtils.scala:387)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.readParquetFootersInParallel(ParquetFileFormat.scala:1186)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1(ParquetFileFormat.scala:1236)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$mergeSchemasInParallel$1$adapted(ParquetFileFormat.scala:1228)
        at org.apache.spark.sql.execution.datasources.SchemaMergeUtils$.$anonfun$mergeSchemasInParallel$2(SchemaMergeUtils.scala:80)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:874)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:874)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:378)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:333)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
        at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:174)
        at org.apache.spark.scheduler.Task.run(Task.scala:152)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:632)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
        at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:96)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:635)
        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
        ... 1 more
Caused by: org.apache.spark.SparkException: [CANNOT_READ_FILE_FOOTER] Could not read footer for file: s3://sungs3bucket/i94_apr16_sub.sas7bdat. Please ensure that the file is in either ORC or Parquet format. If not, please convert it to a valid format. If the file is in the valid format, please check if it is corrupt. If it is, you can choose to either ignore it or fix the corruption.
        at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFooterForFileError(QueryExecutionErrors.scala:1058)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:1199)
        at org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)
        at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
        at scala.util.Success.$anonfun$map$1(Try.scala:255)
        at scala.util.Success.map(Try.scala:213)
        at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
        at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
        at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
        at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)
        at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)
        at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)
        at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)
        at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)
        at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)
Caused by: org.apache.parquet.io.InvalidParquetFormatException: s3://sungs3bucket/i94_apr16_sub.sas7bdat is not a Parquet file. Expected magic number at tail, but found [0, 0, 0, 0]
        at org.apache.parquet.hadoop.ParquetFileReader.getEncryptedFooterMode(ParquetFileReader.java:689)
        at org.apache.parquet.hadoop.ParquetFileReader.readInterFooter(ParquetFileReader.java:583)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:678)
        at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:673)
        at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:935)
        at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:922)
        at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:760)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:198)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:83)
        at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$.$anonfun$readParquetFootersInParallel$1(ParquetFileFormat.scala:1193)
        ... 14 more

25/05/28 08:38:12 INFO SparkContext: Invoking stop() from shutdown hook
25/05/28 08:38:12 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/05/28 08:38:12 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-51-54.us-west-2.compute.internal:4040
25/05/28 08:38:12 INFO YarnClientSchedulerBackend: Interrupting monitor thread
25/05/28 08:38:12 INFO YarnClientSchedulerBackend: Shutting down all executors
25/05/28 08:38:12 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down
25/05/28 08:38:12 INFO YarnClientSchedulerBackend: YARN client scheduler backend Stopped
25/05/28 08:38:12 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/05/28 08:38:12 INFO MemoryStore: MemoryStore cleared
25/05/28 08:38:12 INFO BlockManager: BlockManager stopped
25/05/28 08:38:12 INFO BlockManagerMaster: BlockManagerMaster stopped
25/05/28 08:38:12 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/05/28 08:38:12 INFO SparkContext: Successfully stopped SparkContext
25/05/28 08:38:12 INFO ShutdownHookManager: Shutdown hook called
25/05/28 08:38:12 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-117e82d2-43b0-4742-905c-f3dbbb3025aa/pyspark-9801a7c8-008f-4e4e-9d41-d21c7a0b8ccd
25/05/28 08:38:12 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-117e82d2-43b0-4742-905c-f3dbbb3025aa
25/05/28 08:38:12 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-05f3be05-51fe-46b8-9998-dbec33f2f187
